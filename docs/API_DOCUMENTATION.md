# AstraForge IDE - API Documentation\n\n## Overview\n\nAstraForge IDE is a revolutionary VS Code extension that transforms project ideas into fully functional applications using multi-agent LLM collaboration, vector-based context retrieval, and automated workflow management.\n\n## Core Components\n\n### LLMManager\n\nHandles multi-provider LLM integration with support for parallel processing and intelligent conferencing.\n\n#### Constructor\n```typescript\nnew LLMManager()\n```\nInitializes the LLM manager with configuration from VS Code settings.\n\n#### Methods\n\n##### queryLLM(index: number, prompt: string): Promise<string>\nQueries a specific LLM by panel index.\n\n**Parameters:**\n- `index`: Index of the LLM in the panel (0-based)\n- `prompt`: The prompt to send to the LLM\n\n**Returns:** Promise resolving to the LLM's response\n\n**Example:**\n```typescript\nconst response = await llmManager.queryLLM(0, \"Explain quantum computing\");\nconsole.log(response);\n```\n\n##### conference(prompt: string): Promise<string>\nParallel conferencing with all configured LLMs for collaborative responses.\n\n**Parameters:**\n- `prompt`: The discussion prompt to send to all LLMs\n\n**Returns:** Promise resolving to a formatted discussion transcript\n\n**Example:**\n```typescript\nconst discussion = await llmManager.conference(\"Design a scalable web application\");\n```\n\n##### voteOnDecision(prompt: string, options: string[]): Promise<string>\nDemocratic voting among LLMs with fuzzy string matching for accuracy.\n\n**Parameters:**\n- `prompt`: The decision context\n- `options`: Array of voting options\n\n**Returns:** Promise resolving to the winning option\n\n**Example:**\n```typescript\nconst winner = await llmManager.voteOnDecision(\n  \"Choose the best database\",\n  [\"PostgreSQL\", \"MongoDB\", \"Redis\"]\n);\n```\n\n### VectorDB\n\nProvides semantic search and context retrieval using embeddings.\n\n#### Constructor\n```typescript\nnew VectorDB(storagePath: string)\n```\n\n**Parameters:**\n- `storagePath`: Directory path for persistent storage\n\n#### Methods\n\n##### init(): Promise<void>\nInitializes the vector database and loads existing data.\n\n##### addEmbedding(key: string, vector: number[], metadata: any): Promise<void>\nAdds a new embedding to the database.\n\n**Parameters:**\n- `key`: Unique identifier for the embedding\n- `vector`: The embedding vector (typically 384-dimensional)\n- `metadata`: Associated metadata object\n\n##### queryEmbedding(vector: number[], topK: number): Promise<VectorItem[]>\nQueries for similar embeddings using cosine similarity.\n\n**Parameters:**\n- `vector`: Query embedding vector\n- `topK`: Number of top results to return (default: 5)\n\n**Returns:** Array of vector items with similarity scores\n\n##### getEmbedding(text: string): Promise<number[]>\nGenerates embeddings for text using Hugging Face API with fallback.\n\n**Parameters:**\n- `text`: Input text to embed\n\n**Returns:** Promise resolving to embedding vector\n\n### WorkflowManager\n\nOrchestrates AI-powered development workflows with RL optimization.\n\n#### Constructor\n```typescript\nnew WorkflowManager(\n  llmManager: LLMManager,\n  vectorDB: VectorDB,\n  gitManager: GitManager\n)\n```\n\n#### Methods\n\n##### startWorkflow(idea: string, option?: string): Promise<void>\nInitiates a complete development workflow from idea to deployment.\n\n**Parameters:**\n- `idea`: The project idea description\n- `option`: Optional processing mode (\"letPanelDecide\" for LLM refinement)\n\n**Example:**\n```typescript\nawait workflowManager.startWorkflow(\n  \"Build a real-time chat application with authentication\",\n  \"letPanelDecide\"\n);\n```\n\n##### proceedToNextPhase(): void\nAdvances to the next phase in the workflow (Planning → Prototyping → Testing → Deployment).\n\n### AdaptiveWorkflowRL\n\nReinforcement learning system for workflow optimization.\n\n#### Methods\n\n##### getBestAction(state: WorkflowState): WorkflowAction\nUses Q-learning to recommend the best action for current state.\n\n##### updateQValue(state: WorkflowState, action: WorkflowAction, reward: number, nextState: WorkflowState): void\nUpdates the Q-table based on action outcomes.\n\n##### calculateReward(oldState: WorkflowState, action: WorkflowAction, newState: WorkflowState, success: boolean, userFeedback?: number): number\nCalculates reward signals for learning optimization.\n\n### CollaborationServer\n\nReal-time collaboration server for multi-agent coordination.\n\n#### Methods\n\n##### start(port: number): Promise<void>\nStarts the Socket.IO collaboration server.\n\n##### registerAgent(agent: AgentSession): string\nRegisters a new agent in the collaboration system.\n\n##### broadcastToWorkspace(workspaceId: string, type: string, data: any): void\nBroadcasts messages to all agents in a workspace.\n\n## Configuration\n\n### VS Code Settings\n\nConfigure AstraForge through VS Code settings:\n\n```json\n{\n  \"astraforge.llmPanel\": [\n    {\n      \"provider\": \"OpenAI\",\n      \"key\": \"your-api-key\",\n      \"model\": \"gpt-4\",\n      \"role\": \"primary\"\n    },\n    {\n      \"provider\": \"Anthropic\",\n      \"key\": \"your-claude-key\",\n      \"model\": \"claude-3-sonnet\",\n      \"role\": \"secondary\"\n    }\n  ]\n}\n```\n\n### Environment Variables\n\n- `HUGGINGFACE_API_TOKEN`: Optional token for Hugging Face embeddings\n- `NODE_ENV`: Environment setting (development/production)\n\n## Error Handling\n\nAll async methods include comprehensive error handling:\n\n- **LLM API Failures**: Automatic fallbacks and graceful degradation\n- **Embedding API Errors**: Deterministic fallback embeddings\n- **Network Issues**: Retry logic with exponential backoff\n- **Storage Errors**: Graceful failure with user notification\n\n## Performance Considerations\n\n### Parallel Processing\n- LLM conference calls execute in parallel for 3-5x speed improvement\n- Embedding generation uses batch processing with rate limiting\n- Vector similarity calculations are optimized for large datasets\n\n### Memory Management\n- Vector database uses efficient storage with configurable limits\n- Q-table pruning prevents unbounded memory growth\n- Automatic garbage collection for unused embeddings\n\n### Rate Limiting\n- Built-in delays for API compliance\n- Configurable batch sizes for embedding requests\n- Exponential backoff for failed requests\n\n## Security\n\n### API Key Management\n- Keys stored securely in VS Code settings\n- No hardcoded credentials in source code\n- Optional encryption for sensitive configurations\n\n### Network Security\n- HTTPS enforcement for all API calls\n- Input validation and sanitization\n- Rate limiting to prevent abuse\n\n## Event System\n\n### Workflow Events\n- `phase_started`: Fired when a new phase begins\n- `phase_completed`: Fired when a phase completes successfully\n- `workflow_completed`: Fired when entire workflow finishes\n- `error_occurred`: Fired on workflow errors\n\n### Collaboration Events\n- `agent_joined`: New agent joins workspace\n- `agent_left`: Agent leaves workspace\n- `message_received`: New collaboration message\n- `decision_made`: Collaborative decision reached\n\n## Advanced Usage\n\n### Custom RL Rewards\n```typescript\nconst customReward = workflowRL.calculateReward(\n  currentState,\n  action,\n  newState,\n  success,\n  userFeedback // 0-1 scale\n);\n```\n\n### Vector Database Queries\n```typescript\nconst contextEmbedding = await vectorDB.getEmbedding(\"project context\");\nconst similarProjects = await vectorDB.queryEmbedding(contextEmbedding, 10);\n```\n\n### Multi-Agent Collaboration\n```typescript\nconst server = new CollaborationServer();\nawait server.start(3000);\nserver.registerAgent({\n  id: 'agent-1',\n  type: 'llm',\n  name: 'GPT-4',\n  capabilities: ['planning', 'coding'],\n  status: 'active',\n  workspaceId: 'workspace-1'\n});\n```\n\n## Integration Examples\n\n### With VS Code Extensions\n```typescript\n// In your extension's activate function\nexport async function activate(context: vscode.ExtensionContext) {\n  const llmManager = new LLMManager();\n  const vectorDB = new VectorDB(context.extensionUri.fsPath);\n  await vectorDB.init();\n  \n  const workflowManager = new WorkflowManager(llmManager, vectorDB, gitManager);\n  \n  // Register commands\n  context.subscriptions.push(\n    vscode.commands.registerCommand('astraforge.startWorkflow', async () => {\n      const idea = await vscode.window.showInputBox({\n        prompt: 'Describe your project idea'\n      });\n      if (idea) {\n        await workflowManager.startWorkflow(idea);\n      }\n    })\n  );\n}\n```\n\n### With External APIs\n```typescript\n// Custom LLM provider integration\nclass CustomLLMProvider {\n  async query(prompt: string): Promise<string> {\n    // Your custom implementation\n    return await fetch('/your-api/chat', {\n      method: 'POST',\n      body: JSON.stringify({ prompt })\n    }).then(r => r.text());\n  }\n}\n```\n\n## Troubleshooting\n\n### Common Issues\n\n1. **LLM API Errors**\n   - Verify API keys in settings\n   - Check network connectivity\n   - Review rate limits\n\n2. **Vector DB Performance**\n   - Monitor embedding cache size\n   - Consider index optimization\n   - Check storage permissions\n\n3. **Workflow Stalls**\n   - Review RL exploration rate\n   - Check user feedback quality\n   - Verify phase transition logic\n\n### Debug Mode\n```typescript\n// Enable debug logging\nconsole.log('LLM Response:', response);\nconsole.log('Vector Similarity:', similarity);\nconsole.log('RL Action:', action);\n```\n\n## Version Compatibility\n\n- **VS Code**: ^1.85.0\n- **Node.js**: ^18.0.0\n- **TypeScript**: ^5.0.0\n\n## License\n\nMIT License - see LICENSE file for details.\n